<!DOCTYPE HTML>
<html>
	<head>
		<title>Сверточная нейронная сеть на базе ResNet для автоматической диагностики малярии</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/blog.css" />
		<link href="../assets/css/prism.css" rel="stylesheet" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
							<header id="header">
								<p id="headerlogo"><a href="#" class="logo"><strong>Mayorov</strong>Blog</a></p>
								<ul class="icons">
									<li><a title="About Me" href="https://noreederek.github.io" class="icon fa-address-card"><span class="label">About Me</span></a></li>
									<li><a title="My GitHub" href="https://github.com/noreederek" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
									<li><a title="Telegram" href="tg://resolve?domain=Jim_Root" class="icon brands alt fa-telegram-plane"><span class="label">Telegram</span></a></li>
								</ul>
							</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h2>Сверточная нейронная сеть на базе ResNet для автоматической диагностики малярии</h2>
									</header>
									<div class="chip">
										<div class="chip-content">09 Августа 2018</div>
									</div>

									<p> В данной статье я приведу простой пример того, как можно использовать методики глубокого обучения (а в частности, CNN и библиотеку Keras) для анализа 
										изображений при автоматическом тестировании на малярию. Статья вдохновлена двумя работами - 
										<a href="https://ieeexplore.ieee.org/document/7897215" target="_blank">Evaluations of CNNs for Auto-Identiﬁcation of Malaria Infected Bloodcells</a> (Y. Dong и др) 
										<strong>2017 года</strong> и <a href="https://ieeexplore.ieee.org/document/7822567" target="_blank">CNN-Based 
											Analysis for Malaria Diagnostic</a> (Z. Liang и др.) от <strong>конца 2016</strong> (читайте через SciHub или посмотрите в приложенные мной PDF 
											(<a href="https://drive.google.com/drive/folders/1U1OECwsjVKiapN6ZvxQjchXV54cub7xX" target="_blank">Google Drive</a>)). 
											В работе <strong>2017</strong> года при классификации сравниваются три популярные на тот момент архитектуры нейронных сетей (LeNet, AlexNet and GoogLeNet) на их 
											собственном закрытом датасете из около <em>3500 изображений эритроцитов</em>. В работе же <strong>2016</strong> используется уже открытый датасет из более 
											чем <em>27000 изображений</em> и модифицированная архитектура AlexNet.</p>
									<p>Хотя Y. Dong и удалось достичь точности около 98% на GoogLeNet, их набор данных значительно меньше. Поэтому, чтобы не сравнивать теплое с мягким, при сопоставлении я 
										буду отталкиваться от работы Z. Liang с их точностью на AlexNet около 97%. Я попробую использовать при обучении более современную архитектуру ResNet, чтобы достичь 
										более высокой точности модели при классификации клеток (вероятно при использовании какого-либо ансамбля архитектур, можно было бы добиться больших показателей, но это 
										отрицательно скажется, как на скорости обучения, так и на конечном размере модели). Затем мы упакуем НС в мобильное приложение и протестируем ее производительность 
										локально на смартфоне без подключения к сети.</p>
									<hr class="major" />
									<span class="image fit"><img src="images/malaria_1.jpg" alt="" /></span>
									<p class="italictext">Показатели точности нейросетей Dong и Liang</p>
									<p>Дабы не капитанствовать и не лить много воды, описывать этиологию и эпидемиологию малярии я не буду. Лишь скажу, что микроскопия толстого и тонкого мазков крови – самый 
										распространенный метод ее диагностики, особенно в странах третьего мира (другие методики: определение антител к возбудителю малярии, определение белков возбудителя, 
										ПЦР, экспресс-тесты и др.)</p>
									<p>Также стоит отметить, что существуют современные микроскопические комплексы для автоматической диагностики малярии, например, 
										<a href="https://doc.westmedica.com/47EN/focus_brochures/vision_hema_malaria_rev_7.0_12.2016_focus_brochure_en.pdf">Vision Hema® Malaria</a>, 
										в которых также используются методы машинного обучения. Но вышеуказанные работы и дальнейший техрепорт показывают, что можно уместить функционал 
										на 400 строк кода на ладони, а не внедрять его в диагностический комбайн.</p>
										<span class="image fit"><img src="images/malaria_2.jpg" alt="" /></span>
									<p>
										Итак, теперь определимся с целями. Я постараюсь:<br>
										- Обучить нейросеть c архитектурой ResNet на данных из работы Z. Liang определять пораженные клетки c сопоставимой (а лучше с более высокой) точностью.
										<br>
										- Подробно описать структуру проекта и сети.
										<br>
										- Упаковать НС в приложение и запустить не в облаке, а локально на устройстве (благо Keras позволяет это легко сделать).
										<br>
										- <s>Добавить автоматическое распознавание клеток в поле зрения, осуществлять их подсчет и степень паразитемии</s> (upd: на этом этапе мне стало лень).
									</p>

									<h2>Датасет</h2>
									<p>Датасет представлен снимками отдельных эритроцитов с тонких мазков крови в архиве <em>cell_image.zip</em> 
										(<a href="https://drive.google.com/drive/folders/1U1OECwsjVKiapN6ZvxQjchXV54cub7xX" target="_blank">Google Drive</a>) 
										от <strong>NIH School of Medicine University of Missouri, Columbia</strong>.</p>
										<span class="image fit"><img src="images/malaria_3.jpg" alt="" /></span>
									<p>Набор данных состоит из 27 588 изображений, распределенных специалистом по двум классам: Зараженные (Parasitized) и Незараженные (Uninfected) (по 13 794 файла в каждом).</p>

										<p>В принципе, большинство изображений даже без специализированных знаний можно отнести к верному классу. Хотя, как можно видеть, в данных представлено и множество 
										неоднозначных на первый взгляд образцов.</p>
									
									
									<h2>Требуемый софт</h2>
									<p>

· Keras (высокоуровневый фреймворк для машинного обучения) - 1) pip install tensorflow; 2) pip install keras<br><br>

· NumPy (библиотека для работы с многомерными массивами и высокоуровневыми математическими функциями) – pip install numpy<br><br>

· Scikit-learn (библиотека для анализа данных на python) – pip install scikit<br><br>

· Matplotlib (визуализация данных) - pip install matplotlib
									</p>
									<h2>Структура проекта</h2>
									<p>1) Создаем папку project<br><br>

										2) Копируем в нее copy_datasamples.py (скрипт для разметки данных) и training.py (скрипт для обучения классификатора)<br><br>
										
										3) Создаем в project папку dataset; распаковываем в нее cell_image.zip</p>
										<h2>Подготовка данных (copy_datasamples.py)</h2>
										<p>Открываем copy_datasamples.py</p>
										<span class="image fit"><img src="images/malaria_4.jpg" alt="" /></span>

										<p>Так как наш датасет еще не размечен, нам придется его разделить на Обучающую (по которой будет произведена настройка (оптимизация параметров) модели зависимости), 
											Тестовую (по которой будет оценивается качество построенной модели) и Проверочную (по которой осуществляется выбор наилучшей модели из множества моделей, 
											построенных по обучающей) выборки.</p>
											<span class="image fit"><img src="images/malaria_5.jpg" alt="" /></span>
										<p>Разделяем данные: Обучающие - 72%, тестовые – 20%, проверочные – 8% (Можно задать и другие значения, но это один из общепринятых вариантов).</p>
										<span class="image fit"><img src="images/malaria_6.jpg" alt="" /></span>
										<p>Перемешиваем пути изображений из общей директории, сохраняя распределение по признакам, используем random.seed, чтобы можно было в последующем воспроизвести результат и менять обучающие наборы 
											(<em>модуль random – это генератор псевдослучайных чисел, в котором стандартно для имплементации функции используется системное время, но если задать seed мы сможем
												 потом повторно воспроизвести результаты перемешивания</em>).

											<br><br>Выполняем (жмем Run – Shift + F10) скрипт – и данные разместятся по папкам (Если папок нет, они создадутся).
											
											<br><br>Получим такую структуру данных:</p>
										<span class="image fit"><img src="images/malaria_7.jpg" alt="" /></span>
										<p>Теперь можно приступать к обучению сети.</p>

										<h2>Описание архитектуры нейросети ResNet (training.py)</h2>
										<p>
											Краеугольным камнем ResNet является, предсказывающий отклонения весов от прошлых слоев, остаточный модуль, который был разработан подразделением 
											Microsoft Research Asia. Он был использован на турнире по анализу изображений ImageNet в ансамбле из 6-ти ResNet сетей глубиной в 152 слоя и достиг 
											top-5 ошибки всего в 3.57%. Подробнее можно почитать <a href="https://habr.com/ru/company/mailru/blog/311706/">здесь</a> (архитектура описана хоть и поверхностно, но вполне понятно), хотя для 
											ТруДатаСайнсЭкспириенс лучше полистать оригинальную статью (<a href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition - He, Zhang</a> – Microsoft Research Asia).
										</p>
										<span class="image fit"><img src="images/malaria_8.jpg" alt="" /></span>
										<p>Существует несколько вариантов архитектуры модуля, но наиболее эффективной и точной является архитектура с применением одновременно и пре-активации (т.е. RELU и BN помещается до CONV), 
											и bottleneck. Ее мы и имплементируем.</p>
											<span class="image fit"><img src="images/malaria_9.jpg" alt="" /></span>
										<p>Начинаем мы с импорта нашего, в принципе, стандартного набора классов и функций при сборке сверточной нейронной сети. Тем не менее, стоит обратить
											 внимание на строку 28, где мы импортировали функцию <strong>add</strong>. Внутри остаточного модуля нам нужно будет сложить выходные данные от двух ветвей, тогда 
											 она нам и понадобится. Также в 29 строке мы импортируем функцию <strong>l2-регуляризации</strong>. L2-регуляризация чрезвычайно важна для коррекции весов в ResNet, 
											 так как из-за глубины сети она склонна к переобучению.</p>
											 <span class="image fit"><img src="images/malaria_10.jpg" alt="" /></span>
											<p>Теперь переходим к нашему остаточному модулю (<strong>residual_module</strong>):</p>
											<span class="image fit"><img src="images/malaria_11.jpg" alt="" /></span>
										<p>
											Конкретно эта ResNet была вдохновлена ее Caffe реализацией от самого Kayming He, а также mxnet реализацией от Вей Ву, поэтому мы будем как можно ближе следовать их выбору параметров. Рассмотрим параметры:<br><br>

— <strong>data</strong> - это просто вход в остаточный модуль.<br><br>

— Значение <strong>K</strong> определяет число фильтров, которые будут использоваться в конечном CONV в bottleneck. Первые два слоя CONV будут использовать K / 4 (согласно Kayming He).<br><br>

— <strong>stride</strong> - шаг свертки.<br><br>

— Затем у нас есть параметр <strong>chanDim</strong>, который определяет ось при выполнении batch нормализации – на это значение мы будем ссылаться позже в build функции в зависимости от того, как расположим цветовые каналы в изображении «с начала» или «с конца».<br><br>

— Следующий параметр будет ответственен за уменьшение размеров нашего пространственного объема – логический параметр <strong>red</strong> (то есть, reduce - уменьшение) будет контролировать, уменьшаем ли мы пространственные измерения (True) или нет (False).<br><br>

— Затем мы подаем силу регуляризации <strong>reg</strong> для всех CONV слоев. Параметр bnEps ответственен за избежание ошибок «деления на ноль» при нормализации входов (input normalization). В Керасе по умолчанию 0.001, однако для нашей конкретной реализации мы ее уменьшим. Как и BnMom, который контролирует импульс перемещения медианных значений - это значение обычно по умолчанию равно 0.99, но He и Вэй Ву рекомендуют уменьшить его значение до 0.9.
										</p>
										<p>Теперь, когда параметры residual_module определены, давайте перейдем к телу:</p>
										<span class="image fit"><img src="images/malaria_12.jpg" alt="" /></span>
										<p>В строке 39 мы инициализируем<span>&nbsp;</span><strong>shortcut</strong><span>&nbsp;</span>&ndash; он является просто ссылкой на входные данные. Позже мы добавим<span>&nbsp;</span><strong>shortcut</strong><span>&nbsp;</span>к выводу нашей ветви &laquo;<em>bottleneck + пре-активация</em>&raquo;.</p>
<p>Первая ветка &laquo;<em>пре-активация+bottleneck</em>&raquo; показана в строках 42-46. Здесь мы применяем<span>&nbsp;</span><strong>batch normalization</strong><span>&nbsp;</span>за которой идет активация ReLU, а затем CONV 1*1, с использованием четверти К-фильтров. Также можно заметить, что мы исключаем смещения из наших CONV через<span>&nbsp;</span><strong>use_bias</strong><span>&nbsp;</span>=<span>&nbsp;</span><em>False</em>. Зачем нам намеренно его отключать? По мнению Kayming He, смещения уже находятся в слоях BN, которые непосредственно следуют за свертками, поэтому нет необходимости вводить второе смещение.</p>
<p>Далее у нас идет второй слой CONV в<span>&nbsp;</span><em>bottleneck</em><span>&nbsp;</span>(3*3):</p>
<span class="image fit"><img src="images/malaria_13.jpg" alt="" /></span>
<p>Последний блок (1*1) использует целое К:</p>
<span class="image fit"><img src="images/malaria_14.jpg" alt="" /></span>
<p>Следующий шаг - посмотреть, нужно ли нам уменьшать пространственные размеры. Если придется, мы сделаем это с помощью сверточного слоя со<span>&nbsp;</span><strong>stride</strong><span>&nbsp;</span>&gt; 1 и<span>&nbsp;</span><strong>K</strong>=1.</p>
<p>Вывод окончательного conv3 в<span>&nbsp;</span><em>bottleneck</em><span>&nbsp;</span>складывается вместе с<span>&nbsp;</span><strong>shortcut</strong><span>&nbsp;</span>через add. Он и будет возвращен из<span>&nbsp;</span><strong>residual_module</strong>:</p>
<span class="image fit"><img src="images/malaria_15.jpg" alt="" /></span>
<p><span>Наконец,</span><strong><span>&nbsp;</span>residual_module</strong><span>&nbsp;описан &ndash; он будет служить нашим строительным блоком при создании глубокой сети внутри метода сборки (</span><strong>build</strong><span>):</span></p>
<span class="image fit"><img src="images/malaria_16.jpg" alt="" /></span>
<p>Так же, как и<strong><span>&nbsp;</span>residual_module</strong><span>&nbsp;</span>наша функция сборки требует довольно много параметров для настройки. Но их я опишу на конкретных цифрах, когда мы будем билдить нашу сеть.</p>
<p>Далее, меняем наши<span>&nbsp;</span><strong>inputShape</strong><span>&nbsp;</span>и<span>&nbsp;</span><strong>chanDim</strong><span>&nbsp;</span>(чтобы избежать ошибок из-за форматирования изображений (размеров, порядка значений в цветовых каналах)) в зависимости от того, используем мы &laquo;<em>channels last</em>&raquo; или &laquo;<em>channels first</em>&raquo;:</p>
<span class="image fit"><img src="images/malaria_17.jpg" alt="" /></span>
<p>Теперь мы можем определить вход в нашу ResNet:</p>
<span class="image fit"><img src="images/malaria_18.jpg" alt="" /></span>
<p><span>В отличие от других архитектур нейронных сетей, где первый уровень обычно это CONV, мы видим, что&nbsp;</span><em>ResNet</em><span>&nbsp;использует&nbsp;</span><strong>BN</strong><span>&nbsp;в качестве первого уровня.&nbsp;</span><strong>Batch Normalization</strong><span>&nbsp;для входа - это дополнительный уровень нормализации. Выполнение&nbsp;</span><strong>BN</strong><span>&nbsp;на самом входе иногда помогает устранить необходимость применения&nbsp;</span><em>Mean Normalization</em><span>&nbsp;ко входам.</span></p>
<span class="image fit"><img src="images/malaria_19.jpg" alt="" /></span>
<p><span>В строке 106 мы проходим по массиву уровней. Имейте в виду, что каждая запись в нем представляет собой целочисленное значение, указывающее, сколько остаточных модулей будет сложено поверх каждого следующего.&nbsp;</span><em>ResNet</em><span>&nbsp;попытается уменьшить использование пула насколько это возможно, полагаясь на слои CONV, чтобы уменьшить пространственные размеры. Чтобы уменьшить размер тома без объединения слоев, мы должны установить шаг свертки на строке 107. Если это первый вход на уровне, мы установим шаг в (1, 1), указывая, что не следует выполнять понижающую выборку. Тем не менее, для каждого последующего этапа мы будем применять&nbsp;</span><strong>residual module</strong><span>&nbsp;с шагом (2, 2), что позволит нам уменьшить размер тома. Затем проходим по слоям в строке 112 с уже фиксированным шагом (1,1).</span></p>
<span class="image fit"><img src="images/malaria_20.jpg" alt="" /></span>	
<p><span>Чтобы избежать использования плотных полностью связанных слоев, мы будем применять&nbsp;</span><strong>average pooling</strong><span>&nbsp;для уменьшения размера тома до&nbsp;</span><em>1 х 1 х classes</em><span>:</span></p>							
<span class="image fit"><img src="images/malaria_21.jpg" alt="" /></span>
<p><span>Мы применяем&nbsp;</span><em>Softmax Activation</em><span>&nbsp;(</span><a href="https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86dv" target="_blank" title="https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d">подробно</a><span>) для получения конечных вероятностей на выходе. Полностью построенная модель ResNet затем возвращается в вызывающую функцию в строке 113.</span></p>
<span class="image fit"><img src="images/malaria_22.jpg" alt="" /></span>
<h2>Обучение сети (training.py)</h2>
<p>Импортируем пакеты:</p>
<span class="image fit"><img src="images/malaria_23.jpg" alt="" /></span>
<p>Теперь давайте установим наши параметры обучения и определим нашу функцию снижения скорости обучения:</p>
<span class="image fit"><img src="images/malaria_24.jpg" alt="" /></span>
<p>В строках 143-145 мы определяем число эпох, начальную скорость обучения и размер пакета.</p>
<span class="image fit"><img src="images/malaria_25.jpg" alt="" /></span>
<p>Я обнаружил, что обучение с<span>&nbsp;</span><strong>NUM_EPOCHS</strong><span>&nbsp;</span>= 50 работает вполне неплохо (хотя думаю хватит и 20, прирост в точности на 50 не такой уж значительный).<span>&nbsp;</span><strong>BS</strong><span>&nbsp;</span>(размер пакета) = 32 подходит даже для обучения на ЦП, если достаточно терпения (на хорошей GPU можно поставить 64 или выше). Наш<span>&nbsp;</span><strong>INIT_LR</strong><span>&nbsp;</span>= 1e-1 (начальная скорость обучения) будет уменьшаться в соответствии с функцией<span>&nbsp;</span><strong>poly_decay</strong>.</p>
<p>Наша функция<span>&nbsp;</span><strong>poly_decay</strong><span>&nbsp;</span>определена в строках 150-157. Эта функция поможет нам изменять<span>&nbsp;</span><em>learning rate</em><span>&nbsp;</span>после каждой эпохи. Мы устанавливаем мощность = 1,0, которая по сути превращает наш полиномиальный распад в линейный распад. Выходные данные мы отправим в последующем в<span>&nbsp;</span><strong>callback</strong><span>&nbsp;</span>-<span>&nbsp;</span><em>Learning Rate Scheduller</em>.</p>
<span class="image fit"><img src="images/malaria_26.jpg" alt="" /></span>
<p><span>Далее, давайте возьмем количество изображений в обучающих, проверочных и тестовых наборах, чтобы отобразить их в статистике и рассчитать число шагов в эпохе, используем код из</span><strong><span>&nbsp;</span>copy_datasamples.py</strong><span>:</span></p>
<span class="image fit"><img src="images/malaria_27.jpg" alt="" /></span>
<p>Далее мы используем аугментацию данных:</p>
<span class="image fit"><img src="images/malaria_28.jpg" alt="" /></span>
<p>Аугментация (дополнение, расширение) данных (<strong>Data Augmentation</strong>) включает в себя широкий спектр методов, используемых для создания новых обучающих образцов. По сути, мы применяем различные методы случайного незначительного трансформинга (трансляция, ротация и др.) изображений (без изменения метки класса данных) для повышения обобщающей способности модели. Учитывая, что наша сеть постоянно видит новые, слегка модифицированные версии входных данных, она способна генерить более надежные функции (на тестовые выборки эта методика не применяется). В большинстве случаев мы получим увеличение точности тестирования, хотя иногда и за счет небольшого падения точности обучения.</p>
<p>В следующем блоке мы создим<span>&nbsp;</span><em>Keras генераторы</em>, используемые для загрузки изображений из входного каталога.</p>
<span class="image fit"><img src="images/malaria_29.jpg" alt="" /></span>
<p>Важно помнить, что функция<span>&nbsp;</span><strong>flow_from_directory<span>&nbsp;</span></strong>предполагает:</p>
<p>- Наличие базового входного каталога для данных.</p>
<p>- Внутри этого базового входного каталога есть N подкаталогов, где каждый подкаталог соответствует метке класса.</p>
<p>Снабжаем генераторы параметрами:</p>
<p>-<span>&nbsp;</span><strong>class_mode - categorical</strong></p>
<p>- Изменяем размер всех изображений до<span>&nbsp;</span><em>64 х 64</em><span>&nbsp;</span>пикселей.</p>
<p>- Устанавливаем цветовой канал (<strong>color_mode</strong>) "rgb".</p>
<p>- Перемешиваем пути изображений,<span>&nbsp;</span><em>но только для тренировочного генератора!</em></p>
<p>- Используем<span>&nbsp;</span><strong>BS</strong><span>&nbsp;</span>= 32.</p>
<p>Теперь устанавливаем<span>&nbsp;</span><strong>ResNet</strong><span>&nbsp;</span>и описываем модель (строка 219):</p>

<span class="image fit"><img src="images/malaria_30.jpg" alt="" /></span>
<p>- Изображения имеют размер<span>&nbsp;</span><em>64 x 64 x 3</em><span>&nbsp;</span>(3-канала RGB).</p>
<p>- Класса всего 2 (<em>Инфицированные и неинфицированные</em>).</p>
<p>- ResNet выполнит (<em>3, 4, 6</em>) укладку с (<em>64, 128, 256, 512</em>) CONV слоями, подразумевая, что:</p>
<p>= Первый слой CONV в ResNet, до уменьшения пространственных размеров, будет иметь всего 64 фильтра.</p>
<p>= Затем мы сложим 3 набора остаточных модулей. Три слоя CONV в каждом остаточном модуле получат 32, 32 и 128 фильтров CONV соответственно.</p>
<p>=Далее мы собираем 4 набора остаточных модулей, где каждый из трех уровней CONV будет иметь 64, 64 и 256 фильтров.</p>
<p>=Наконец, мы объединяем 6 наборов остаточных модулей, где каждый уровень CONV получает 128, 128 и 512 фильтров. Затем применяется<span>&nbsp;</span><strong>SoftMax</strong>.</p>
<p>В строке 220 инициализируется оптимизатор SGD (<a href="https://habr.com/ru/post/318970/">Градиентный спуск</a>) с начальным<span>&nbsp;</span><strong>learning rate</strong><span>&nbsp;</span>1e-1 и<strong><span>&nbsp;</span>momentum</strong><span>&nbsp;</span>(почти тоже, что BnMom) 0,9.</p>
<p>В строках 221 и 222 мы используем в качестве функции потерь бинарную кроссэнтропию (<strong>binary_crossentropy</strong>), поскольку мы выполняем классификацию по 2 меткам. Для более чем двух классов я бы использовал<span>&nbsp;</span><em>categorycal_crossentropy</em>.</p>
<p>На строке 225 мы задаем<strong><span>&nbsp;</span>callback</strong>. Callback&rsquo;и выполняются в конце каждой эпохи. Мы применяем наш<span>&nbsp;</span><strong>poly_decay</strong><span>&nbsp;</span><em>LearningRateScheduler<span>&nbsp;</span></em>для выгрузки статистики и изменения скорости обучения после каждой эпохи.</p>
<span class="image fit"><img src="images/malaria_31.jpg" alt="" /></span>
<p>Вызов<span>&nbsp;</span><strong>model.fit_generator</strong><span>&nbsp;</span>в строках 226-232 заставляет наш скрипт начать процесс обучения. Генератор<strong><span>&nbsp;</span>trainGen<span>&nbsp;</span></strong>автоматически (1) загрузит наши изображения с диска и (2) проанализирует метки классов. То же самое будет выполнять<span>&nbsp;</span><strong>valGen</strong>, но только для проверочной выборки.</p>
<p>Теперь, когда эта модель обучена, мы можем оценить ее на тестовом наборе. Сбрасываем тестовый генератор перед оценкой, если этого не сделать, получим неверный отчет.</p>
<span class="image fit"><img src="images/malaria_32.jpg" alt="" /></span>
<p>Для оценки нашей модели мы сделаем прогнозы на тестовых данных и затем найдем метку с наибольшей вероятностью для каждого изображения в тестовой выборке (Строки 238-243).</p>
<span class="image fit"><img src="images/malaria_33.jpg" alt="" /></span>
<p>Затем отображаем отчет о классификации (строки 245-247). И с помощью matplotlib отображаем наш график точности и ошибки по отношению к эпохам. Сохраняем модель.</p>
<span class="image fit"><img src="images/malaria_34.jpg" alt="" /></span>
<h2>Оценка результатов обучения</h2>
<p>Давайте оценим результаты нашего обучения</p>
<span class="image fit"><img src="images/malaria_35.jpg" alt="" /></span>
<span class="image fit"><img src="images/malaria_36.jpg" alt="" /></span>
<p>На облачной GPU Tesla K80 процесс обучения занял чуть больше 2 часов. Мы добились точности<span>&nbsp;</span><strong>98.13% на тренировочной</strong>,<span>&nbsp;</span><strong>98.26% на обучающей</strong><span>&nbsp;</span>и<span>&nbsp;</span><strong>98% на тестовой выборке<span>&nbsp;</span></strong>(<em>хех, это я еще с random.seed не баловался</em>), а соотношения на графике указывают на валидность модели. В общем, мы перебили результат AlexNet и даже сравнялись с GoogLeNet на большей выборке (теплое с мягким). Объем конечной модели без квантования весов составил около 16 МБ и теперь мы упакуем его в приложение.</p>
<p>Далее почти не будет исходников, я лишь пролью свет на некоторые нюансы в сборке локальной модели, а затем приложу скрины и описание тестирования некоторых данных в приложении.</p>
<h2>Мобильное приложение</h2>
<p>Для создания локальной модели нужно использовать<span>&nbsp;</span><a href="https://firebase.google.com/products/ml">Google ML Kit for Firebase (документация)</a>, модель нужно сохранить сначала в формате pb через (<em>session = K.get_session(); K.set_session(session)</em>). Затем конвертировать в формат TFlite через два скрипта (<em>freeze_graph --input_binary=false --input_graph=trained_model.pbtxt --output_node_names=result/Softmax --output_graph=freeze_model.pb --input_checkpoint=final_model.ckpt</em>). А в<span>&nbsp;</span><strong>build.gradle</strong><span>&nbsp;</span>добавить запись<span>&nbsp;</span><em>aaptOptions { noCompress "tflite" }</em>, чтобы отключить сжатие. Подробности в<span>&nbsp;</span><a href="https://firebase.google.com/products/ml">документации</a>. Процесс создания apk можно опустить, так как к основной теме статьи он не относится.</p>
<p>В общем, в конце концов я апнул нашу модель в apk. Но добавлять автоматическое определение клеток и их подсчет мне было уже совсем скучно &ndash; задача возможно и нетривиальная (<em>скорее муторная</em>), но вполне решаемая с помощью OpenCV. Мне кажется, можно будет обойтись исключительно компьютерным зрением, без использования машинного обучения,<span>&nbsp;</span><em>хотя я бы и сюда его добавил</em>:</p>
<span class="image fit"><img src="images/malaria_37.jpg" alt="" /></span>
<p>Итак, так как автоматически наши изображения не кропаются, я нашел снимок поля зрения для анализа в интернете и ручками выделил в нем 50 клеток, из которых 45 нормальных и 5 зараженных (<em>ИМХО конечно, я не уверен</em>).</p>
<p>Вот такие результаты получились в приложении без подключения к интернету (<em>удивительно, но обработалось все очень быстро, хотя у меня и довольно бюджетный девайс</em>):</p>
<span class="image fit"><img src="images/malaria_38.jpg" alt="" /></span>
<p>Нейросеть успешно классифицировала все 50 изображений, и, по сути, сравнялась со мной в точности определения инфицированных клеток (<em>не знаю хорошо это или плохо)), я даже кубический от призматического эпителия сейчас не отличу</em>). Также пробовал закидывать данные из выборок и получал сопоставимые с<span>&nbsp;</span><strong>98%</strong><span>&nbsp;</span>точностью ответы.</p>
<p>В итоге большая часть поставленных в начале целей достигнута. Подобные техники можно использовать для внедрения методик автоматической классификации практически в любой сфере лабораторной диагностики, при условии конечно достаточного объема грамотно собранных и размеченных обучающих данных.</p>
<p><em>Статьи, архив с датасетом и файлы приложу к публикации и на<span>&nbsp;</span><a href="https://drive.google.com/drive/folders/1U1OECwsjVKiapN6ZvxQjchXV54cub7xX">Google Drive (Скачать)</a>. Также позже добавлю в GDrive .ipynb файл и ссылку на Google Colab.</em></p>


</section>
								
								<ul class="icons">
									<li><strong>Поделиться: </strong></li>
									<li><a title="Share with Facebook" href="javascript: void(0)" class="icon brands alt fa-facebook"
										onClick="window.open('https://www.facebook.com/sharer.php?u=' + window.location.href,'sharer','status=0,toolbar=0,width=650,height=500');">
									<span class="label">Facebook</span></a></li>
									<li><a title="Share with VK" href="javascript: void(0)" class="icon brands alt fa-vk"
										onClick="window.open('https://vkontakte.ru/share.php?url=' + window.location.href,'sharer','status=0,toolbar=0,width=650,height=500');">
									<span class="label">VK</span></a></li>
									<li><a title="Telegram" href="javascript: void(0)" class="icon brands alt fa-telegram-plane"
										onClick="window.open('https://telegram.me/share/url?url=' + window.location.href,'sharer','status=0,toolbar=0,width=650,height=500');">
									<span class="label">Telegram</span></a></li>
									<li><a title="Twitter" href="javascript: void(0)" class="icon brands alt fa-twitter"
										onClick="window.open('https://twitter.com/intent/tweet?text=' + 'Hey, checkout this post! ' + window.location.href,'sharer','status=0,toolbar=0,width=650,height=500');">
									<span class="label">Twitter</span></a></li>
								</ul>

						</div>
					</div>

				<!-- Sidebar -->
				<div id="sidebar">
					<div class="inner">
						<html-include src="../blog_sidepanel.html"></html-include>
					</div>
				</div>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/prism.js"></script>
			<script src="../assets/js/blog.js"></script>
			<script src="../assets/js/html-include.js"></script>

	</body>
</html>